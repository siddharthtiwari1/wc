% Experiments
\label{sec:experiments}

\subsection{Experimental Setup}

\subsubsection{Hardware Platform}

TurtleBot 4 base (differential drive, 0.26m/s max speed) equipped with:
\begin{itemize}
    \item RPLidar S3: 360° 2D LiDAR, 40m range, 10Hz scan rate
    \item RealSense D455: 1280×720 RGB-D @30fps, FOV 87°×58°
    \item Jetson AGX Orin: 64GB RAM, NVIDIA Ampere GPU (275 TOPS)
\end{itemize}

\subsubsection{Environments}

\begin{table}[h]
\centering
\caption{Real-world evaluation environments}
\label{tab:environments}
\begin{tabular}{lccc}
\toprule
Environment & Area (m²) & Objects & Scenes \\
\midrule
Homes & 80-120 & 50-80 & 3 \\
Offices & 60-100 & 40-60 & 2 \\
Labs & 100-150 & 60-90 & 1 \\
Public (lobby) & 200 & 30-50 & 1 \\
\midrule
\textbf{Total} & - & \textbf{400+} & \textbf{7} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Simulation}: 20 Matterport3D scenes via Habitat~\cite{habitat19iccv} for
scalability analysis.

\subsubsection{Evaluation Dataset}

\begin{itemize}
    \item \textbf{Real robot}: 110 natural language instructions (60 single-step,
    50 multi-step with 2-4 subgoals)

    \item \textbf{Simulation}: 200 instructions across 20 scenes (CapNav used 9
    instructions on 1 scene~\cite{capnav2026})

    \item \textbf{Dynamic scenes}: 30 instructions with objects moved mid-navigation
\end{itemize}

Example instructions:
\begin{itemize}
    \item \textit{``Go to the red mug on the wooden table near the window''}
    \item \textit{``Navigate to the white sofa with curved edges, then go to the
    black chair with armrests''}
    \item \textit{``Find the striped pillow on the grey couch in the living room''}
\end{itemize}

\subsubsection{Baselines}

\begin{enumerate}
    \item \textbf{VLMaps}~\cite{huang2023vlmaps}: Open-vocab mapping (category-only)
    \item \textbf{O3D-SIM}~\cite{nanwani2024o3dsim}: Instance-level retrieval
    (adapted for real robot)
    \item \textbf{LM-Nav}~\cite{shah2023lmnav}: LLM-based landmark navigation
    \item \textbf{CapNav (sim)}~\cite{capnav2026}: Attribute-aware (simulation)
    \item \textbf{RAN (sim)}: Our method in simulation (fair comparison)
    \item \textbf{RAN (real)}: Our method on real robot
\end{enumerate}

\subsection{Main Results}

\subsubsection{Real-Robot Navigation Performance}

\begin{table}[h]
\centering
\caption{Subgoal success rate (\%) on real robot}
\label{tab:ssr_real}
\begin{tabular}{lcccc}
\toprule
Method & Home & Office & Lab & \textbf{Avg} \\
\midrule
VLMaps & 48.2 & 52.1 & 45.6 & 48.6 \\
O3D-SIM & 55.3 & 58.7 & 51.2 & 55.1 \\
LM-Nav & 52.7 & 56.4 & 49.8 & 53.0 \\
\midrule
\textbf{RAN (Ours)} & \textbf{72.4} & \textbf{75.8} & \textbf{68.3} & \textbf{72.2} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key findings}:
\begin{itemize}
    \item \textbf{+23.6\% absolute gain} over VLMaps
    \item \textbf{+17.1\%} over O3D-SIM (instance-level baseline)
    \item Best performance in homes (72.4\%) due to cluttered, attribute-rich scenes
\end{itemize}

\subsubsection{Long-Horizon Navigation}

\begin{table}[h]
\centering
\caption{Full-chain completion (\%) on multi-step instructions}
\label{tab:fcc}
\begin{tabular}{lcccc}
\toprule
Method & 2-step & 3-step & 4-step & \textbf{Avg} \\
\midrule
VLMaps & 32.1 & 22.4 & 12.8 & 22.4 \\
CapNav (sim)~\cite{capnav2026} & - & - & 20.0 & - \\
\midrule
\textbf{RAN (Ours)} & \textbf{78.5} & \textbf{61.2} & \textbf{42.9} & \textbf{60.9} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Impact of hierarchical verification}:
\begin{itemize}
    \item \textbf{+42.9\%} on 4-step instructions vs VLMaps
    \item \textbf{+22.9\%} vs CapNav (20\% on 4-step~\cite{capnav2026})
    \item Error accumulation reduced by cascading coarse-to-fine checks
\end{itemize}

\subsection{Ablation Studies}

\begin{table}[h]
\centering
\caption{Ablation: Component contributions to SSR (\%)}
\label{tab:ablation}
\begin{tabular}{lc}
\toprule
Configuration & SSR \\
\midrule
Full System & \textbf{72.2} \\
\midrule
\textit{w/o} Confidence weighting & 68.5 (-3.7) \\
\textit{w/o} Hierarchical verification & 59.4 (-12.8) \\
\textit{w/o} Adaptive thresholds & 66.1 (-6.1) \\
\textit{w/o} Dynamic updates & 65.8 (-6.4) \\
\textit{w/o} Level-4 re-verification & 63.2 (-9.0) \\
\midrule
CLIP-only (no DINOv2) & 64.7 (-7.5) \\
Fixed thresholds (CapNav-style) & 67.3 (-4.9) \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key insights}:
\begin{itemize}
    \item Hierarchical verification contributes \textbf{12.8\%} (largest gain)
    \item Level-4 re-verification adds \textbf{9.0\%} (catches false positives)
    \item DINOv2+CLIP fusion improves \textbf{7.5\%} over CLIP-only
\end{itemize}

\subsection{Sim-to-Real Transfer}

\begin{table}[h]
\centering
\caption{Sim-to-real transfer analysis}
\label{tab:sim2real}
\begin{tabular}{lcc}
\toprule
Method & Sim (Habitat) & Real & Gap \\
\midrule
CapNav~\cite{capnav2026} & 71.0 & - & - \\
VLMaps & 62.3 & 48.6 & -13.7 \\
\midrule
\textbf{RAN (Ours)} & \textbf{80.9} & \textbf{72.2} & \textbf{-8.7} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analysis}:
\begin{itemize}
    \item \textbf{Only 8.7\% drop} from sim to real (vs 13.7\% for VLMaps)
    \item Confidence estimation helps: Low-confidence predictions (blur, depth
    holes) are down-weighted
    \item Adaptive thresholds: Sim uses lower $\tau_{\text{sem}}$ (clean data);
    real auto-tunes higher (noisy observations)
\end{itemize}

\subsection{Dynamic Scene Handling}

\begin{table}[h]
\centering
\caption{Performance when objects move mid-navigation}
\label{tab:dynamic}
\begin{tabular}{lcc}
\toprule
Method & Static & Dynamic (moved) \\
\midrule
VLMaps & 48.6 & 21.3 (-27.3) \\
RAN \textit{w/o} updates & 72.2 & 38.7 (-33.5) \\
\midrule
\textbf{RAN (full)} & \textbf{72.2} & \textbf{64.4} (\textbf{-7.8}) \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Recovery mechanism}:
\begin{itemize}
    \item \textbf{89.2\% recovery rate} (64.4 / 72.2) when objects move
    \item Online updates detect stale instances (Level-4 fails → re-query)
    \item VLMaps drops \textbf{27.3\%} (no update mechanism)
\end{itemize}

\subsection{Qualitative Analysis}

\subsubsection{Success Cases}

\textbf{Example 1}: \textit{``Go to the white sofa with curved edges''}
\begin{itemize}
    \item Scene: Living room with 3 sofas (2 white, 1 grey)
    \item Baseline (VLMaps): Navigates to wrong white sofa (straight edges)
    \item RAN: Verifies shape attribute → Selects correct curved sofa
\end{itemize}

\textbf{Example 2}: \textit{``Find the striped pillow on the grey couch''}
\begin{itemize}
    \item Scene: 2 grey couches, 1 with striped pillow, 1 with solid pillow
    \item RAN: Multi-view fusion confirms stripes → Correct couch
    \item Confidence: 0.87 (high due to 5 good views)
\end{itemize}

\subsubsection{Failure Cases}

\textbf{Failure 1}: \textit{``Go to the transparent glass vase''}
\begin{itemize}
    \item Cause: Depth sensor fails on transparent objects
    \item Confidence: 0.23 (low $q_{\text{depth}}$)
    \item System response: ``I'm not confident'' (uncertainty estimation)
\end{itemize}

\textbf{Failure 2}: \textit{``Find the laptop on the cluttered desk''}
\begin{itemize}
    \item Cause: Laptop occluded by papers (only 10\% visible)
    \item Level-2 verification rejects (shape confidence < 0.3)
    \item Could be fixed with active exploration (future work)
\end{itemize}

\subsection{Timing Analysis}

\begin{table}[h]
\centering
\caption{System latency breakdown}
\label{tab:timing}
\begin{tabular}{lcc}
\toprule
Component & Time (s) & \% Total \\
\midrule
Map building (offline) & 540 & - \\
\quad Detection (YOLO) & 0.12 & - \\
\quad Segmentation (SAM2) & 0.31 & - \\
\quad Feature extraction & 0.18 & - \\
\quad Clustering & 45 & - \\
\midrule
Online navigation (per query) & \textbf{2.3} & \textbf{100} \\
\quad Language parsing (LLM) & 0.6 & 26.1 \\
\quad Retrieval (FAISS) & 0.05 & 2.2 \\
\quad Hierarchical verification & 0.8 & 34.8 \\
\quad Path planning (Nav2) & 0.85 & 37.0 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Efficiency notes}:
\begin{itemize}
    \item Map building: 8-10 min for 100m² (1,200 frames)
    \item Query latency: 2.3s (acceptable for indoor navigation)
    \item Bottleneck: LLM parsing (0.6s via API; can be reduced with local LLM)
\end{itemize}

\subsection{Comparison to CapNav}

\begin{table}[h]
\centering
\caption{Direct comparison to CapNav~\cite{capnav2026}}
\label{tab:vs_capnav}
\begin{tabular}{lcc}
\toprule
Metric & CapNav & RAN \\
\midrule
Evaluation scenes & 1 (sim) & 7 (real) + 20 (sim) \\
Subgoal success (sim) & 71.0\% & \textbf{80.9\%} (+9.9) \\
Full-chain (4-step) & 20.0\% & \textbf{42.9\%} (+22.9) \\
Real robot & ✗ & ✓ \\
Baselines & 1 & 4 \\
Ablations & 0 & 5 \\
Dynamic scenes & ✗ & ✓ (89.2\% recovery) \\
Uncertainty estimation & ✗ & ✓ \\
Adaptive thresholds & ✗ & ✓ \\
\bottomrule
\end{tabular}
\end{table}
