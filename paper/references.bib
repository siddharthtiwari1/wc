% Key References for Attribute-Aware Navigation Paper

@inproceedings{anderson2018vlnr2r,
  title={Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments},
  author={Anderson, Peter and Wu, Qi and Teney, Damien and Bruce, Jake and Johnson, Mark and S{\"u}nderhauf, Niko and Reid, Ian and Gould, Stephen and Van Den Hengel, Anton},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{huang2023vlmaps,
  title={Visual language maps for robot navigation},
  author={Huang, Chenguang and Mees, Oier and Zeng, Andy and Burgard, Wolfram},
  booktitle={ICRA},
  year={2023}
}

@article{capnav2026,
  title={CapNav: Towards robust indoor navigation with description-first maps},
  author={Vatsi, Ritali and Vaidande, Ayush Ravindra and Sharma, Vikas and Tiwari, Siddharth and Shukla, Amit},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025},
  note={Under review at ICLR 2026}
}

@inproceedings{nanwani2024o3dsim,
  title={Open-set 3d semantic instance maps for vision language navigation},
  author={Nanwani, Laksh and Gupta, Kumaraditya and Mathur, Aditya and Agrawal, Swayam and Hafez, AH Abdul and Krishna, K Madhava},
  booktitle={arXiv preprint arXiv:2404.17922},
  year={2024}
}

@inproceedings{kerr2023lerf,
  title={LERF: Language embedded radiance fields},
  author={Kerr, Justin and Kim, Chung Min and Goldberg, Ken and Kanazawa, Angjoo and Tancik, Matthew},
  booktitle={ICCV},
  pages={19729--19739},
  year={2023}
}

@inproceedings{jatavallabhula2023conceptfusion,
  title={Conceptfusion: Open-set multimodal 3d mapping},
  author={Jatavallabhula, Krishna Murthy and Kuwajerwala, Alihusein and Gu, Qiao and Omama, Mohd and Chen, Tao and Maalouf, Alaa and Li, Shuang and Iyer, Ganesh and Saryazdi, Soroush and Keetha, Nikhil and others},
  booktitle={RSS},
  year={2023}
}

@inproceedings{peng2023openscene,
  title={Openscene: 3d scene understanding with open vocabularies},
  author={Peng, Songyou and Genova, Kyle and Jiang, Chiyu and Tagliasacchi, Andrea and Pollefeys, Marc and Funkhouser, Thomas and others},
  booktitle={CVPR},
  pages={815--824},
  year={2023}
}

@inproceedings{takmaz2023openmask3d,
  title={Openmask3d: Open-vocabulary 3d instance segmentation},
  author={Takmaz, Ay{\c{c}}a and Fedele, Elisabetta and Sumner, Robert W and Pollefeys, Marc and Tombari, Federico and Engelmann, Francis},
  booktitle={NeurIPS},
  volume={36},
  year={2023}
}

@inproceedings{shafiullah2023clipfields,
  title={Clip-fields: Weakly supervised semantic fields for robotic memory},
  author={Shafiullah, Nur Muhammad Mahi and Wang, Jianren and Mart{\'\i}n-Mart{\'\i}n, Roberto and Wu, Jiajun and Liu, C Karen and Fei-Fei, Li},
  booktitle={RSS},
  year={2023}
}

@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={ICML},
  pages={12888--12900},
  year={2022}
}

@inproceedings{li2023blip2,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={ICML},
  pages={19730--19742},
  year={2023}
}

@article{oquab2024dinov2,
  title={Dinov2: Learning robust visual features without supervision},
  author={Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others},
  journal={arXiv preprint arXiv:2304.07193},
  year={2023}
}

@inproceedings{zhang2024longclip,
  title={Long-clip: Unlocking the long-text capability of clip},
  author={Zhang, Beichen and Zhang, Pan and Dong, Xiaoyi and Zang, Yuhang and Wang, Jiaqi},
  booktitle={ECCV},
  year={2024}
}

@article{yoloworld2024,
  title={YOLO-World: Real-time open-vocabulary object detection},
  author={Cheng, Tianheng and Song, Lin and Ge, Yixiao and Liu, Wenyu and Wang, Xinggang and Shan, Ying},
  journal={arXiv preprint arXiv:2401.17270},
  year={2024}
}

@article{sam2024,
  title={Segment anything 2},
  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and others},
  journal={arXiv preprint arXiv:2408.00714},
  year={2024}
}

@inproceedings{habitat19iccv,
  title={Habitat: A platform for embodied ai research},
  author={Savva, Manolis and Kadian, Abhishek and Maksymets, Oleksandr and Zhao, Yili and Wijmans, Erik and Jain, Bhavana and Straub, Julian and Liu, Jia and Koltun, Vladlen and Malik, Jitendra and others},
  booktitle={ICCV},
  pages={9339--9347},
  year={2019}
}

@inproceedings{zhou2023navgpt,
  title={NavGPT: Explicit reasoning in vision-and-language navigation with large language models},
  author={Zhou, Gengze and Hong, Yicong and Wu, Qi},
  booktitle={arXiv preprint arXiv:2305.16986},
  year={2023}
}

@inproceedings{pan2024langnav,
  title={Langnav: Language as a perceptual representation for navigation},
  author={Pan, Bowen and Panda, Rameswar and Jin, SouYoung and Feris, Rogerio and Oliva, Aude and Isola, Phillip and Kim, Yoon},
  booktitle={NAACL},
  pages={950--974},
  year={2024}
}

@article{martins2024ovoslam,
  title={Ovo-slam: Open-vocabulary online simultaneous localization and mapping},
  author={Martins, Tomas Berriel and Oswald, Martin R and Civera, Javier},
  journal={arXiv preprint arXiv:2411.15043},
  year={2024}
}

@article{laina2025findanything,
  title={Findanything: Open-vocabulary and object-centric mapping for robot exploration in any environment},
  author={Laina, Sebastian Barbas and Boche, Simon and Papatheodorou, Sotiris and Schaefer, Simon and Jung, Jaehyung and Leutenegger, Stefan},
  journal={arXiv preprint arXiv:2504.08603},
  year={2025}
}

@inproceedings{nav2,
  title={The marathon 2: A navigation system},
  author={Macenski, Steven and Mart{\'\i}n, Francisco and White, Ruffin and Clavero, Jonatan Gin{\'e}s},
  booktitle={IROS},
  pages={2718--2725},
  year={2020}
}

@article{llava2024,
  title={Llava-1.6: Improved reasoning, ocr, and world knowledge},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2401.05448},
  year={2024}
}

@inproceedings{shah2023lmnav,
  title={LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action},
  author={Shah, Dhruv and Equi, Blazej and Osiński, Błażej and Ichter, Brian and Levine, Sergey},
  booktitle={CoRL},
  pages={492--504},
  year={2023}
}

@inproceedings{chaplot2020active,
  title={Learning to explore using active neural slam},
  author={Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Saurabh and Gupta, Abhinav and Salakhutdinov, Ruslan},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{kuo2024spatial,
  title={Compositional spatial grounding with vision-language models},
  author={Kuo, Yen-Ling and Katz, Boris and Barbu, Andrei},
  booktitle={CVPR},
  year={2024}
}
