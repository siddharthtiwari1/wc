# Wheelchair Sensor Fusion Package

**Advanced 2D LiDAR and Camera Fusion for Obstacle Avoidance**

![ROS2](https://img.shields.io/badge/ROS2-Jazzy%20|%20Humble-blue)
![Python](https://img.shields.io/badge/Python-3.10+-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

## Overview

This package implements a novel **Adaptive Semantic-Geometric Fusion Algorithm** that combines 2D LiDAR (RPLidar S3) and RGB-D camera (RealSense D455) data with YOLOv11-based object detection for robust obstacle avoidance in wheelchair navigation systems.

### Key Features

- âœ… Real-time sensor fusion at 30 Hz
- âœ… Adaptive weighting based on environmental conditions
- âœ… YOLOv11 integration for semantic understanding
- âœ… DBSCAN-based LiDAR clustering
- âœ… Temporal synchronization of heterogeneous sensors
- âœ… Nav2 integration for obstacle avoidance
- âœ… Comprehensive RViz visualization
- âœ… Open-source and research-ready

### Performance

- **Detection Accuracy (F1)**: 0.93
- **Processing Rate**: 30 Hz
- **False Positive Reduction**: 18% over baseline methods

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RPLidar S3     â”‚â”€â”€â”
â”‚  (2D LiDAR)     â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”œâ”€â”€â”€â–¶â”‚  Sensor Fusion Node  â”‚
â”‚  RealSense D455 â”‚â”€â”€â”¤    â”‚  (Adaptive Fusion)   â”‚
â”‚  (RGB-D Camera) â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚               â”‚
                     â”‚               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YOLOv11        â”‚â”€â”€â”˜    â”‚  Fused Obstacles     â”‚
â”‚  (Detection)    â”‚       â”‚  (Nav2 Compatible)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Sensor Acquisition**: Synchronized capture of LiDAR scans, camera images, and depth data
2. **LiDAR Processing**: DBSCAN clustering to extract obstacle clusters
3. **Object Detection**: YOLOv11 processes RGB images for semantic classification
4. **Sensor Fusion**: Adaptive geometric-semantic fusion algorithm
5. **Obstacle Publishing**: Unified obstacle representation for Nav2

## Installation

### Prerequisites

- **ROS2**: Jazzy or Humble
- **Python**: 3.10+
- **CUDA**: (Optional, for GPU acceleration)
- **Hardware**:
  - RPLidar S3 or compatible 2D LiDAR
  - Intel RealSense D455 (or D435i)
  - NVIDIA GPU recommended for real-time YOLOv11

### System Dependencies

```bash
# ROS2 packages
sudo apt update
sudo apt install -y \
    ros-${ROS_DISTRO}-realsense2-camera \
    ros-${ROS_DISTRO}-realsense2-description \
    ros-${ROS_DISTRO}-rplidar-ros \
    ros-${ROS_DISTRO}-vision-msgs \
    ros-${ROS_DISTRO}-cv-bridge \
    ros-${ROS_DISTRO}-image-transport \
    ros-${ROS_DISTRO}-message-filters \
    ros-${ROS_DISTRO}-tf2-ros \
    ros-${ROS_DISTRO}-nav2-bringup \
    ros-${ROS_DISTRO}-navigation2

# Python dependencies
sudo apt install -y \
    python3-pip \
    python3-opencv \
    python3-numpy \
    python3-sklearn
```

### Python Packages

```bash
# Install Ultralytics YOLO
pip install ultralytics

# Install additional dependencies
pip install \
    opencv-python \
    numpy \
    scikit-learn \
    torch torchvision  # For GPU support
```

### Install RealSense SDK (if not already installed)

```bash
# Add Intel server to apt repositories
sudo mkdir -p /etc/apt/keyrings
curl -sSf https://librealsense.intel.com/Debian/librealsense.pgp | \
  sudo tee /etc/apt/keyrings/librealsense.pgp > /dev/null

echo "deb [signed-by=/etc/apt/keyrings/librealsense.pgp] https://librealsense.intel.com/Debian/apt-repo `lsb_release -cs` main" | \
  sudo tee /etc/apt/sources.list.d/librealsense.list

sudo apt update
sudo apt install -y librealsense2-dkms librealsense2-utils
```

### Build the Package

```bash
# Navigate to your ROS2 workspace
cd ~/ros2_ws  # or your workspace path

# Clone this repository (if not already in workspace)
# git clone <repository-url> src/wheelchair_sensor_fusion

# Install dependencies
rosdep install --from-paths src --ignore-src -r -y

# Build
colcon build --packages-select wheelchair_sensor_fusion

# Source
source install/setup.bash
```

## Usage

### Quick Start

Launch the complete sensor fusion system:

```bash
ros2 launch wheelchair_sensor_fusion complete_system.launch.py
```

This will start:
- RealSense D455 camera driver
- RPLidar S3 driver
- LiDAR processor node
- YOLO detector node
- Sensor fusion node
- Obstacle publisher node
- RViz visualization

### Launch Fusion Only (Assuming sensors already running)

```bash
ros2 launch wheelchair_sensor_fusion sensor_fusion.launch.py
```

### Individual Nodes

#### 1. LiDAR Processor

```bash
ros2 run wheelchair_sensor_fusion lidar_processor_node
```

**Parameters**:
- `scan_topic`: Input LaserScan topic (default: `/scan`)
- `dbscan_eps`: DBSCAN epsilon parameter (default: `0.15`)
- `min_cluster_size`: Minimum points per cluster (default: `3`)

#### 2. YOLO Detector

```bash
ros2 run wheelchair_sensor_fusion yolo_detector_node \
  --ros-args -p model_path:=yolov11n.pt -p device:=cuda
```

**Parameters**:
- `model_path`: YOLO model file (`yolov11n.pt`, `yolov11s.pt`, etc.)
- `device`: `cuda` or `cpu`
- `confidence_threshold`: Detection confidence (default: `0.5`)
- `image_topic`: Camera image topic

**Available Models**:
- `yolov11n.pt` - Nano (fastest, lowest accuracy)
- `yolov11s.pt` - Small
- `yolov11m.pt` - Medium
- `yolov11l.pt` - Large
- `yolov11x.pt` - Extra Large (slowest, highest accuracy)

The model will be automatically downloaded on first run.

#### 3. Sensor Fusion Node

```bash
ros2 run wheelchair_sensor_fusion sensor_fusion_node
```

**Parameters**:
- `adaptive_weighting`: Enable adaptive fusion weights (default: `true`)
- `distance_threshold`: Distance for weight transition (default: `2.0`)
- `fusion_iou_threshold`: IoU threshold for matching (default: `0.3`)

#### 4. Obstacle Publisher

```bash
ros2 run wheelchair_sensor_fusion obstacle_publisher_node
```

Publishes obstacles as OccupancyGrid for Nav2 integration.

## Configuration

Edit configuration files in `config/`:

### `lidar_processor.yaml`

```yaml
lidar_processor:
  ros__parameters:
    scan_topic: '/scan'
    dbscan_eps: 0.15
    min_range: 0.15
    max_range: 6.0
```

### `sensor_fusion.yaml`

```yaml
sensor_fusion:
  ros__parameters:
    adaptive_weighting: true
    distance_threshold: 2.0
    fusion_iou_threshold: 0.3
```

## Topics

### Subscribed Topics

| Topic | Type | Description |
|-------|------|-------------|
| `/scan` | `sensor_msgs/LaserScan` | LiDAR scan data |
| `/camera/color/image_raw` | `sensor_msgs/Image` | RGB image |
| `/camera/aligned_depth_to_color/image_raw` | `sensor_msgs/Image` | Aligned depth image |
| `/camera/color/camera_info` | `sensor_msgs/CameraInfo` | Camera calibration |

### Published Topics

| Topic | Type | Description |
|-------|------|-------------|
| `/lidar/clusters` | `visualization_msgs/MarkerArray` | LiDAR obstacle clusters |
| `/yolo/detections` | `vision_msgs/Detection2DArray` | YOLO detections |
| `/yolo/debug_image` | `sensor_msgs/Image` | Annotated debug image |
| `/fusion/obstacles` | `visualization_msgs/MarkerArray` | Fused obstacles |
| `/fusion/visualization` | `visualization_msgs/MarkerArray` | Detailed visualization |
| `/fusion/costmap` | `nav_msgs/OccupancyGrid` | Costmap for Nav2 |

## Visualization

### RViz

The default RViz configuration shows:
- **Grid**: Reference frame
- **TF**: Transform tree
- **LiDAR Scan**: Raw laser scan
- **LiDAR Clusters**: Clustered obstacles
- **Camera Image**: RGB feed
- **YOLO Debug**: Annotated detections
- **Fused Obstacles**: Final fused obstacles
- **Costmap**: Obstacle costmap

Color coding for fused obstacles:
- ðŸŸ£ **Purple**: Fused (LiDAR + Camera)
- ðŸŸ¢ **Green**: LiDAR only
- ðŸ”µ **Blue**: Camera only

## Algorithm Details

### Adaptive Fusion Weights

The fusion algorithm computes adaptive weights based on:

```python
w_lidar = 0.3 + 0.7 * sigmoid(distance - threshold)
w_camera = 0.3 + 0.7 * (1 - w_lidar) * confidence
```

- **Close range (< 2m)**: Favor camera depth and semantics
- **Long range (> 2m)**: Favor LiDAR accuracy
- **Confidence-based**: Higher YOLO confidence increases camera weight

### Association Algorithm

1. Project LiDAR clusters to image coordinates
2. Compute IoU with YOLO bounding boxes
3. Match using greedy assignment (can be upgraded to Hungarian)
4. Create fused obstacles from matched pairs
5. Include unmatched detections from both sensors

## Troubleshooting

### YOLO Model Not Found

```bash
# Download manually
wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov11n.pt
```

### Camera Not Detected

```bash
# Check RealSense devices
rs-enumerate-devices

# Test camera
realsense-viewer
```

### LiDAR Permission Denied

```bash
# Grant USB permissions
sudo chmod 666 /dev/ttyUSB0
```

### Low Frame Rate

- Use smaller YOLO model (`yolov11n.pt` instead of `yolov11x.pt`)
- Enable GPU: `device:=cuda`
- Reduce image resolution in camera launch

### TF Errors

```bash
# Check TF tree
ros2 run tf2_tools view_frames

# Ensure URDF is published
ros2 topic echo /robot_description
```

## Research and Citation

This work implements the algorithm described in:

> **"Novel Adaptive Sensor Fusion Framework for 2D LiDAR and Camera Integration in Wheelchair Obstacle Avoidance Using YOLOv11 and ROS2"**
>
> Siddharth Tiwari, Indian Institute of Technology Mandi, 2025

See `docs/template.tex` for the complete research paper.

### Related Work

- [Benayed et al., 2025] - LiDAR 2D and Camera Fusion for ADAS with YOLOv9
- [Abdullah, 2024] - ROS2 LiDAR Camera Fusion with Detection
- [Zhang et al., 2024] - Semantic Fusion with Contour and Inverse Projection

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## License

This project is licensed under the MIT License - see LICENSE file for details.

## Acknowledgments

- Indian Institute of Technology Mandi for research support
- Ultralytics team for YOLOv11
- Intel for RealSense SDK
- SLAMTEC for RPLidar drivers
- ROS2 and Nav2 communities

## Contact

**Siddharth Tiwari**
- Email: s24035@students.iitmandi.ac.in
- Institution: Indian Institute of Technology Mandi

## References

- [ROS2 Documentation](https://docs.ros.org/)
- [Nav2 Documentation](https://navigation.ros.org/)
- [Ultralytics YOLOv11](https://docs.ultralytics.com/)
- [Intel RealSense](https://www.intelrealsense.com/)
- [RPLidar](https://www.slamtec.com/en/Lidar/A3)

---

**Version**: 1.0.0
**Last Updated**: November 2025
**Status**: Active Development
