# ðŸ“˜ Implementation Guide: Wheelchair Sensor Fusion System

**Companion to**: ICRA2025_SensorFusion.tex
**Author**: Siddharth Tiwari (IIT Mandi)
**Last Updated**: 2025-11-22

---

## ðŸŽ¯ Purpose

This guide provides **step-by-step instructions** for:
1. Installing and deploying the wheelchair sensor fusion system
2. Collecting experimental data for the ICRA 2025 paper
3. Reproducing the results reported in the paper
4. Running evaluations and generating figures

**Companion files**:
- `ICRA2025_SensorFusion.tex` - The research paper
- `ICRA2025_references.bib` - Bibliography
- This guide - Practical implementation

---

## ðŸ“‹ Table of Contents

1. [Hardware Requirements](#1-hardware-requirements)
2. [Software Installation](#2-software-installation)
3. [System Deployment](#3-system-deployment)
4. [Data Collection](#4-data-collection)
5. [Performance Evaluation](#5-performance-evaluation)
6. [Generating Paper Results](#6-generating-paper-results)
7. [Troubleshooting](#7-troubleshooting)

---

## 1. Hardware Requirements

### Required Hardware (as used in paper)

| Component | Model | Specifications | Notes |
|-----------|-------|----------------|-------|
| **2D LiDAR** | RPLidar S3 | 20 Hz, 0.25Â° resolution, 40m max range | Indoor use: 0.25-6m effective |
| **RGB-D Camera** | Intel RealSense D455 | 1280Ã—720 @ 30 fps, 0.4-6m depth | USB 3.0 required |
| **Compute** | Laptop/Workstation | GPU (NVIDIA), 8GB+ RAM | See platform options below |
| **Wheelchair** | Any powered wheelchair | Indoor navigation capable | Sensor mounting required |

### Compute Platform Options

**Option 1: Development Laptop** (Your RTX 5050)
```
GPU: NVIDIA RTX 5050 8GB
CPU: Intel i5-13th Gen HX
RAM: 16GB+
Expected Performance: 30-40 Hz fusion
Power: 50-65W
```

**Option 2: Deployment Workstation** (Your A4000)
```
GPU: NVIDIA A4000 24GB
CPU: Multi-core workstation CPU
RAM: 32GB+
Expected Performance: 40+ Hz fusion
Power: 140W
```

**Option 3: Embedded Platform** (Future)
```
GPU: NVIDIA AGX Orin
RAM: 32GB
Expected Performance: 30 Hz fusion
Power: 35W
```

### Physical Setup

1. **LiDAR Mounting**:
   - Position: Front of wheelchair, chest height (~80-100cm)
   - Orientation: Horizontal scanning plane
   - Coverage: 360Â° scan (full environment awareness)
   - Clearance: No obstructions in scan plane

2. **Camera Mounting**:
   - Position: Front of wheelchair, slightly above LiDAR (~100-120cm)
   - Orientation: Forward-facing, level (not tilted)
   - Field of View: 87Â° horizontal (overlaps with LiDAR)
   - USB: Direct connection to compute (USB 3.0 port)

3. **Compute Mounting**:
   - Position: Secure location on wheelchair frame
   - Cooling: Adequate ventilation for GPU
   - Power: Connected to wheelchair battery (12V â†’ AC inverter)

---

## 2. Software Installation

### System Requirements

- **OS**: Ubuntu 24.04 LTS
- **ROS**: ROS2 Jazzy Jalisco
- **Python**: 3.12 (system)
- **CUDA**: 12.1+ compatible drivers

### One-Command Installation

```bash
# Clone repository
cd ~
git clone https://github.com/siddharthtiwari1/wc.git
cd wc

# Run installation script (15-20 minutes)
cd src/wheelchair_sensor_fusion/scripts
./install_system.sh
```

**What it installs**:
- âœ… ROS2 Jazzy + dependencies
- âœ… Python packages (numpy, opencv, sklearn, scipy, ultralytics)
- âœ… PyTorch with CUDA support
- âœ… YOLOv11 models (yolov11n.pt, yolov11s.pt)
- âœ… RealSense SDK + ROS2 wrapper
- âœ… RPLidar ROS2 driver
- âœ… Builds wheelchair_sensor_fusion package

### Verify Installation

```bash
# Validate all components
./validate_system.sh

# Expected output:
# âœ“ SYSTEM VALIDATION PASSED
# System is ready to launch!
```

### Environment Setup

**No conda/virtualenv needed!** Just source ROS2:

```bash
# Automatically added to ~/.bashrc by install script
source /opt/ros/jazzy/setup.bash
source ~/ros2_ws/install/setup.bash
```

---

## 3. System Deployment

### Step 1: Connect Sensors

1. **Connect RPLidar**:
   ```bash
   # Plug USB cable
   ls /dev/ttyUSB*  # Should show /dev/ttyUSB0

   # Fix permissions if needed
   sudo chmod 666 /dev/ttyUSB0
   # OR permanently:
   sudo usermod -a -G dialout $USER
   # (then logout/login)
   ```

2. **Connect RealSense**:
   ```bash
   # Plug into USB 3.0 port (blue connector)
   rs-enumerate-devices

   # Expected output:
   # Device info:
   #   Name: Intel RealSense D455
   #   Serial Number: XXXXXXXXXX
   #   Firmware Version: X.X.X.X
   ```

### Step 2: Calibrate Sensors

Run the interactive calibration wizard:

```bash
cd ~/wc/src/wheelchair_sensor_fusion/scripts
./calibrate_sensors.sh
```

**This will**:
- âœ… Verify sensor connectivity
- âœ… Launch sensor nodes
- âœ… Check data streams
- âœ… Validate TF transforms
- âœ… Provide calibration checklist

**Update transforms** in `config/wheelchair_integration.yaml`:

```yaml
static_transform_publisher:
  # Measure these values for YOUR wheelchair
  lidar_transform:
    translation: [0.3, 0.0, 0.8]  # x=30cm forward, z=80cm height
    rotation: [0, 0, 0, 1]        # No rotation (quaternion)

  camera_transform:
    translation: [0.3, 0.0, 1.0]  # x=30cm forward, z=100cm height
    rotation: [0, 0, 0, 1]        # No rotation (quaternion)
```

### Step 3: Launch Fusion System

```bash
# Terminal 1: Launch complete system
ros2 launch wheelchair_sensor_fusion wheelchair_fusion.launch.py

# Terminal 2: Visualize in RViz
rviz2 -d ~/wc/src/wheelchair_sensor_fusion/rviz/sensor_fusion.rviz

# Terminal 3: Monitor performance
cd ~/wc/src/wheelchair_sensor_fusion/scripts
python3 monitor_performance.py
```

### Step 4: Verify Operation

Check the performance monitor output (Terminal 3):

```
===============================================================================
PERFORMANCE REPORT
===============================================================================
Topic Rates (Hz):
  /scan                                    : 20.12 Hz âœ“
  /camera/color/image_raw                  : 29.87 Hz âœ“
  /yolo/detections                         : 26.34 Hz âœ“
  /fusion/obstacles                        : 28.91 Hz âœ“

Fusion Latency:
  Average: 32.45 ms
  Min:     18.23 ms
  Max:     47.89 ms
  Status:  âœ“ GOOD

Current Mode: FULL_FUSION

System Resources:
  CPU:      45.23 %
  Memory:   1823.45 MB
  Threads:  24
  GPU:      52.34 %
  GPU Mem:  2341.23 MB
===============================================================================
```

**Success criteria**:
- âœ“ All topics publishing at expected rates
- âœ“ Fusion latency < 50 ms
- âœ“ Mode is FULL_FUSION
- âœ“ GPU is being used (40-60%)

---

## 4. Data Collection

### Experimental Setup (for ICRA Paper)

The paper reports results from **5 indoor environments**:

1. **Office Corridor** (30m Ã— 2m)
   - Obstacles: People, furniture, doorways
   - Lighting: Fluorescent overhead
   - Duration: 10 minutes

2. **Laboratory** (10m Ã— 8m)
   - Obstacles: Lab equipment, tables, chairs
   - Lighting: Mixed (windows + artificial)
   - Duration: 15 minutes

3. **Lobby** (15m Ã— 12m)
   - Obstacles: People, plants, glass doors
   - Lighting: Natural (windows)
   - Duration: 10 minutes

4. **Narrow Hallway** (20m Ã— 1.5m)
   - Obstacles: Wall-mounted items, people
   - Lighting: Low (dim corridor)
   - Duration: 8 minutes

5. **Cluttered Room** (8m Ã— 6m)
   - Obstacles: Furniture, boxes, mixed objects
   - Lighting: Variable
   - Duration: 12 minutes

### Data Collection Procedure

#### A. Record ROS2 Bag

```bash
# Create data directory
mkdir -p ~/wheelchair_data/environment_1_office

# Start recording
cd ~/wheelchair_data/environment_1_office
ros2 bag record \
  /scan \
  /camera/color/image_raw \
  /camera/aligned_depth_to_color/image_raw \
  /camera/color/camera_info \
  /yolo/detections \
  /yolo/debug_image \
  /fusion/obstacles \
  /fusion/status \
  /fusion/diagnostics \
  /tf \
  /tf_static

# Navigate wheelchair through environment for 10 minutes
# Stop with Ctrl+C when done
```

#### B. Log Ground Truth

While recording, manually log ground truth obstacles:

```bash
# Create ground truth file
cat > ground_truth.txt << EOF
# Timestamp, Object Type, X, Y, Width, Height
0.0, person, 2.5, 0.0, 0.5, 1.8
5.2, chair, 3.1, -0.8, 0.6, 0.9
10.5, table, 4.2, 0.5, 1.2, 0.8
...
EOF
```

#### C. Record Metadata

```bash
cat > metadata.yaml << EOF
environment: office_corridor
date: 2025-11-22
duration_sec: 600
lighting: fluorescent
obstacles:
  - person (dynamic)
  - chair (static)
  - table (static)
  - doorway (static)
wheelchair_speed: 0.5 m/s
notes: Morning session, moderate foot traffic
EOF
```

### Repeat for All Environments

```bash
# Collect data for all 5 environments
~/wheelchair_data/
â”œâ”€â”€ environment_1_office/
â”‚   â”œâ”€â”€ rosbag2_*.db3
â”‚   â”œâ”€â”€ ground_truth.txt
â”‚   â””â”€â”€ metadata.yaml
â”œâ”€â”€ environment_2_lab/
â”œâ”€â”€ environment_3_lobby/
â”œâ”€â”€ environment_4_hallway/
â””â”€â”€ environment_5_cluttered/
```

---

## 5. Performance Evaluation

### Replay Recorded Data

```bash
# Launch fusion system
ros2 launch wheelchair_sensor_fusion wheelchair_fusion.launch.py

# In another terminal, replay bag
cd ~/wheelchair_data/environment_1_office
ros2 bag play rosbag2_*
```

### Automated Evaluation

Run the evaluation script:

```bash
cd ~/wc/src/wheelchair_sensor_fusion/scripts
python3 evaluate_performance.py \
  --bag-dir ~/wheelchair_data/environment_1_office \
  --ground-truth ~/wheelchair_data/environment_1_office/ground_truth.txt \
  --output ~/wheelchair_data/environment_1_office/results.json
```

**Metrics computed**:
- True Positives, False Positives, False Negatives
- Precision, Recall, F1-score
- Average detection latency
- Topic publishing rates
- Fusion success rate
- Mode transition frequency

### Benchmark System Performance

```bash
# Run 60-second benchmark
cd ~/wc/src/wheelchair_sensor_fusion/scripts
./benchmark_system.sh 60

# Results saved to: /tmp/wheelchair_fusion_benchmark_TIMESTAMP/
# - resources.csv (CPU/GPU/memory over time)
# - BENCHMARK_REPORT.md (summary)
```

---

## 6. Generating Paper Results

### Figure 1: System Overview

**Components**:
- (a) Photo of wheelchair with sensors
- (b) Example navigation scenario
- (c) Fusion visualization from RViz

**How to generate**:

```bash
# Launch system with visualization
ros2 launch wheelchair_sensor_fusion wheelchair_fusion.launch.py
rviz2 -d ~/wc/src/wheelchair_sensor_fusion/rviz/sensor_fusion.rviz

# Take screenshots:
# - In RViz, use File â†’ Export â†’ Screenshot
# - Save as figure1c_fusion_viz.png
```

### Figure 2: Adaptive Weighting

**TikZ plot** (already in LaTeX):
- Distance-based weight function
- Confidence modulation
- Lighting adaptation

**No data needed** - mathematical plot generated from equations.

### Figure 3: Comparative Performance

**Generate from evaluation results**:

```python
# Create plot from results
python3 << 'EOF'
import json
import matplotlib.pyplot as plt

# Load results
results = {
    'Proposed': json.load(open('results/proposed.json')),
    'LiDAR-only': json.load(open('results/lidar_only.json')),
    'Camera-only': json.load(open('results/camera_only.json')),
    'Fixed-weight': json.load(open('results/fixed_weight.json')),
}

# Extract F1 scores
methods = list(results.keys())
f1_scores = [results[m]['f1_score'] for m in methods]

# Plot
plt.figure(figsize=(6, 4))
plt.bar(methods, f1_scores)
plt.ylabel('F1-Score')
plt.ylim([0, 1])
plt.title('Comparative Performance')
plt.tight_layout()
plt.savefig('figure3_comparison.pdf')
EOF
```

### Table I: Comparison with State-of-the-Art

**Data sources**:
- Your results: From evaluation script output
- Baseline results: From published papers (cited in references)

**Format**:

| Method | Sensors | F1-Score | Latency (ms) | Real-time? |
|--------|---------|----------|--------------|------------|
| Benayed et al. [X] | LiDAR+Cam | 0.85 | 120 | No |
| Abdullah et al. [Y] | LiDAR+Cam | 0.82 | 80 | Yes |
| Fixed-weight fusion | LiDAR+Cam | 0.87 | 45 | Yes |
| **Proposed (Ours)** | **LiDAR+Cam+YOLO** | **0.93** | **32** | **Yes** |

### Table II: Ablation Study

**Run experiments**:

```bash
# 1. Full system
ros2 launch wheelchair_sensor_fusion wheelchair_fusion.launch.py

# 2. Without adaptive weighting (fixed weights)
ros2 launch wheelchair_sensor_fusion wheelchair_fusion.launch.py \
  adaptive_weights:=false

# 3. Without confidence modulation
ros2 launch wheelchair_sensor_fusion wheelchair_fusion.launch.py \
  confidence_modulation:=false

# 4. Without lighting adaptation
ros2 launch wheelchair_sensor_fusion wheelchair_fusion.launch.py \
  lighting_adaptation:=false
```

**Collect metrics for each variant**.

### Table III: Computational Performance

**From benchmark results**:

```bash
# Run on each platform
./benchmark_system.sh 60

# Extract metrics:
# - Average CPU %
# - Average GPU %
# - Memory usage
# - Fusion rate (Hz)
```

| Platform | CPU (%) | GPU (%) | Mem (MB) | Rate (Hz) |
|----------|---------|---------|----------|-----------|
| RTX 5050 Laptop | 45 | 52 | 1823 | 30 |
| A4000 Workstation | 32 | 38 | 2145 | 40 |

---

## 7. Troubleshooting

### Common Issues

#### Issue 1: Low Fusion Rate (<25 Hz)

**Diagnosis**:
```bash
python3 monitor_performance.py
# Check which topic is slow
```

**Solutions**:
- **YOLO too slow**: Switch to smaller model
  ```bash
  # Edit config/wheelchair_integration.yaml
  yolo_model: "yolov11n.pt"  # Was: yolov11s.pt
  ```
- **GPU not used**: Check CUDA availability
  ```bash
  python3 -c "import torch; print(torch.cuda.is_available())"
  ```
- **CPU bottleneck**: Reduce image resolution
  ```bash
  # Edit launch file
  image_width: 640  # Was: 1280
  image_height: 480  # Was: 720
  ```

#### Issue 2: False Positives

**Symptoms**: Obstacles detected where none exist

**Solutions**:
- Increase YOLO confidence threshold:
  ```yaml
  yolo_confidence_threshold: 0.6  # Was: 0.5
  ```
- Increase minimum cluster size:
  ```yaml
  min_cluster_points: 5  # Was: 3
  ```
- Check sensor calibration (TF transforms)

#### Issue 3: False Negatives

**Symptoms**: Real obstacles not detected

**Solutions**:
- Decrease YOLO confidence threshold:
  ```yaml
  yolo_confidence_threshold: 0.4  # Was: 0.5
  ```
- Increase max obstacle distance:
  ```yaml
  max_obstacle_distance: 6.0  # Was: 5.0
  ```
- Check sensor mounting (clear field of view?)

#### Issue 4: Frequent Mode Transitions

**Symptoms**: Mode changes between FULL_FUSION and degraded modes

**Diagnosis**:
```bash
ros2 topic echo /fusion/diagnostics
# Check sensor health status
```

**Solutions**:
- Increase sensor timeout:
  ```yaml
  sensor_timeout_sec: 1.0  # Was: 0.5
  ```
- Fix sensor connection issues (USB, power)
- Reduce fusion rate expectation

#### Issue 5: GPU Out of Memory

**Symptoms**: System crashes with CUDA OOM error

**Solutions**:
- **Automatic**: System falls back to CPU (already implemented)
- **Manual**: Use smaller YOLO model
  ```yaml
  yolo_model: "yolov11n.pt"  # Smallest model
  ```
- Reduce batch size (if processing multiple images)

---

## 8. Advanced Usage

### Custom Environments

To test in your own environment:

1. **Update parameters** in `config/wheelchair_integration.yaml`:
   ```yaml
   # Adjust for your space
   max_obstacle_distance: 4.0  # Smaller rooms
   obstacle_inflation: 0.4     # Tighter spaces
   ```

2. **Collect data** following Section 4

3. **Evaluate** following Section 5

### Integration with Nav2

The system publishes costmaps compatible with Nav2:

```bash
# Launch Nav2 with fusion costmap
ros2 launch nav2_bringup navigation_launch.py \
  use_sim_time:=false \
  params_file:=config/nav2_params.yaml

# Costmap configuration in nav2_params.yaml:
local_costmap:
  plugins: ["obstacle_layer"]
  obstacle_layer:
    plugin: "nav2_costmap_2d::ObstacleLayer"
    observation_sources: fusion_costmap
    fusion_costmap:
      topic: /fusion/obstacle_costmap
      clearing: true
      marking: true
```

### Real-time Monitoring Dashboard

Create a custom monitoring dashboard:

```python
# dashboard.py
import rclpy
from rclpy.node import Node
# ... (see scripts/monitor_performance.py for reference)
```

---

## 9. Reproducing Paper Results

### Complete Workflow

1. **Setup** (1 day):
   ```bash
   ./install_system.sh
   ./validate_system.sh
   ./calibrate_sensors.sh
   ```

2. **Collect Data** (1 week):
   - Record bags in 5 environments
   - Log ground truth
   - Record metadata

3. **Run Baselines** (3 days):
   - LiDAR-only mode
   - Camera-only mode
   - Fixed-weight fusion
   - Proposed adaptive fusion

4. **Evaluate** (2 days):
   - Run evaluation scripts
   - Generate metrics
   - Create plots

5. **Generate Figures** (1 day):
   - Export RViz screenshots
   - Create comparison plots
   - Format for LaTeX

6. **Compile Paper** (1 day):
   ```bash
   cd ~/wc/docs
   pdflatex ICRA2025_SensorFusion.tex
   bibtex ICRA2025_SensorFusion
   pdflatex ICRA2025_SensorFusion.tex
   pdflatex ICRA2025_SensorFusion.tex
   ```

**Total time**: ~2 weeks (including data collection)

---

## 10. Citation

If you use this work, please cite:

```bibtex
@inproceedings{tiwari2025adaptive,
  title={Adaptive Semantic-Geometric Sensor Fusion for Safe Indoor Wheelchair Navigation},
  author={Tiwari, Siddharth},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  year={2025},
  organization={IEEE}
}
```

---

## 11. Support

- **Documentation**: See `QUICKSTART.md` for quick setup
- **Hardware**: See `HARDWARE_OPTIMIZATION.md` for platform-specific tuning
- **Environment**: See `ENVIRONMENT_SETUP.md` for installation details
- **Issues**: Open issues on GitHub repository

---

## Appendix: File Checklist

Before submitting paper, ensure you have:

- [ ] LaTeX source (`ICRA2025_SensorFusion.tex`)
- [ ] Bibliography (`ICRA2025_references.bib`)
- [ ] All figures (Figure 1-5 as PDF/PNG)
- [ ] Source code (GitHub repository link)
- [ ] Experimental data (ROS bags, ground truth)
- [ ] Evaluation scripts
- [ ] README with reproduction instructions
- [ ] Video demonstration (optional but recommended)

**Supplementary materials** for reviewers:
- Complete source code
- Example ROS bags
- Evaluation scripts
- Docker container (optional)

---

**Good luck with your ICRA 2025 submission!** ðŸš€

---

*Last updated: 2025-11-22*
*Siddharth Tiwari, IIT Mandi*
*s24035@students.iitmandi.ac.in*
